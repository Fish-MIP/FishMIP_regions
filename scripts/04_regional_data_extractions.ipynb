{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77518d7-f938-4696-adda-d8c0082da34c",
   "metadata": {},
   "source": [
    "# Extracting ESM data using regional model boundaries.\n",
    "**Author:** Denisse Fierro Arcos  \n",
    "**Last updated:** 2024-09-03  \n",
    "  \n",
    "This script needs to be run whenever there is an update of regional boundaries or ESM outputs. This script is best run in NCI's Gadi but file paths can be updated to work anywhere where GFDL outputs are stored.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d1366-b910-4b5a-b59d-0a0d659632a0",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63672164-0233-44d2-b45e-78dbe3beff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zarr\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b1b4e-c988-415c-81b1-07ab6462d69a",
   "metadata": {},
   "source": [
    "## Starting cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda89c5e-3989-4809-864b-2ab338c33fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker = 1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df0dcf-ba5a-4698-9beb-1825c0a2b896",
   "metadata": {},
   "source": [
    "## Loading gridded mask and regions shapefiles\n",
    "We will use these files to extract data for each FishMIP regional model. Note that the gridded mask **MUST** exactly match the ESM where we will extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a96b3e-b5dd-4991-a1b0-35a0b45bd2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading FishMIP regional models shapefile\n",
    "rmes = gpd.read_file('/g/data/vf71/shared_resources/FishMIP_regional_models/FishMIP_regional_models.shp')\n",
    "\n",
    "#Loading FishMIP regional models gridded mask\n",
    "mask_ras = xr.open_dataarray(os.path.join('/g/data/vf71/shared_resources/FishMIPMasks/',\\\n",
    "        'merged_regional_fishmip/gfdl-mom6-cobalt2_areacello_15arcmin_fishMIP_regional_merged.nc'))\n",
    "#Rechunking data to make it more manageable\n",
    "mask_ras = mask_ras.chunk({'lat': 144, 'lon': 288})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ea155-b6c1-4503-a4b3-121bc7abfa89",
   "metadata": {},
   "source": [
    "## Setting up directories and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fee86d5-6ed7-4065-a795-b2bf9e6d3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base folder containing Earth System Model (ESM) data\n",
    "base_dir = '/g/data/vf71/fishmip_inputs/ISIMIP3a/global_inputs/obsclim/025deg'\n",
    "#Get a list of all files containing monthly ESM outputs (depth is excluded)\n",
    "list_files = glob(os.path.join(base_dir, '*monthly*.nc'))\n",
    "\n",
    "#Define (or create) folder for outputs\n",
    "base_out = base_dir.replace('global', 'regional')\n",
    "os.makedirs(base_out, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f2979-2118-4a25-b959-73275b59f3ea",
   "metadata": {},
   "source": [
    "## Defining useful functions for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f1c120-0c26-4d19-b216-0ad6fae779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ard_data(file_path, boolean_mask_ras):\n",
    "    '''\n",
    "    Open netCDF files in analysis ready data (ARD) format. That is apply chunks\n",
    "    that make data analysis easier.\n",
    "    \n",
    "    Inputs:\n",
    "    file_path (character): Full file path where netCDF file is stored.\n",
    "    boolean_mask_ras (boolean data array): Data array to be used as initial mask\n",
    "    to decrease the size of the original dataset. This mask makes no distinction\n",
    "    between regional models, it simply identifies grid cells within regional \n",
    "    model boundaries with the value of 1.\n",
    "    \n",
    "    Outputs:\n",
    "    da (data array): ARD data array containing data only for grid cells within\n",
    "    regional model boundaries.\n",
    "    '''\n",
    "\n",
    "    #Getting chunks from gridded mask to apply it to model data array\n",
    "    [lat_chunk] = np.unique(boolean_mask_ras.chunksizes['lat'])\n",
    "    [lon_chunk] = np.unique(boolean_mask_ras.chunksizes['lon'])\n",
    "    \n",
    "    #Open data array\n",
    "    da = xr.open_dataarray(file_path)\n",
    "    \n",
    "    #Checking if there is a fourth dimension in dataset, it is depth, but it is not\n",
    "    #always called the same\n",
    "    if len(da.dims) > 3:\n",
    "        da = da.chunk({'time': 600, 'lev': 5, 'lat': lat_chunk, 'lon': lon_chunk})\n",
    "        [depth_name] = [d for d in da.dims if d not in ['time', 'lat', 'lon']]\n",
    "        #Changing depth dimension name\n",
    "        da = da.rename({depth_name: 'depth_bin_m'})\n",
    "    else:\n",
    "        da = da.chunk({'time': 600, 'lat': lat_chunk, 'lon': lon_chunk})\n",
    "    \n",
    "    #Apply mask for all regions to decrease dataset size\n",
    "    da = da.where(boolean_mask_ras == 1)\n",
    "    \n",
    "    #Add spatial information to dataset\n",
    "    da.rio.set_spatial_dims(x_dim = 'lon', y_dim = 'lat', inplace = True)\n",
    "    da.rio.write_crs('epsg:4326', inplace = True)\n",
    "    \n",
    "    #Change format of time dimension\n",
    "    new_time = da.indexes['time'].to_datetimeindex()\n",
    "    #Changing time in data array\n",
    "    da['time'] = new_time\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724312d4-e906-45e0-8d79-4274bcc8a613",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_ard_data(ard_da, shp_mask, file_out):\n",
    "    '''\n",
    "    Open netCDF files in analysis ready data (ARD) format. That is apply chunks\n",
    "    that make data analysis easier.\n",
    "    \n",
    "    Inputs:\n",
    "    ard_da (data array): Analysis ready data (ARD) data array produced by the \n",
    "    function \"open_ard_data\"\n",
    "    shp_mask (shapefile): Shapefile containing the boundaries of regional models\n",
    "    file_out (character): Full file path where masked data should be stored.\n",
    "    \n",
    "    Outputs:\n",
    "    No data is returned, but masked file will be stored in specified file path.\n",
    "    '''\n",
    "\n",
    "    #Clip data using regional shapefile\n",
    "    da_mask = ard_da.rio.clip(shp_mask.geometry, shp_mask.crs, drop = True, \n",
    "                              all_touched = True)\n",
    "    #Remove spatial information\n",
    "    da_mask = da_mask.drop_vars('spatial_ref')\n",
    "    da_mask.encoding = {}\n",
    "\n",
    "    #Check file extension included in path to save data\n",
    "    if file_out.endswith('zarr'):\n",
    "        for i, c in enumerate(da_mask.chunks):\n",
    "            if len(c) > 1 and len(set(c)) > 1:\n",
    "                print(f'Rechunking {file_out}.')\n",
    "                print(f'Dimension \"{da_mask.dims[i]}\" has unequal chunks.')\n",
    "                da_mask = da_mask.chunk({da_mask.dims[i]: '200MB'})\n",
    "        da_mask.to_zarr(file_out, consolidated = True, mode = 'w')\n",
    "    if file_out.endswith('parquet'):\n",
    "        #Keep data array attributes to be recorded in final data frame\n",
    "        da_attrs = ard_da.attrs\n",
    "        da_attrs = pd.DataFrame([da_attrs])\n",
    "        if 'time' in ard_da.coords:\n",
    "            ind_wider = ['lat', 'lon', 'time', 'vals']\n",
    "        else:\n",
    "            ind_wider = ['lat', 'lon', 'vals']\n",
    "        #Turn extracted data into data frame and remove rows with NA values\n",
    "        df = da_mask.to_series().to_frame().reset_index().dropna()\n",
    "        #Changing column name to standardise across variables\n",
    "        df = df.rename(columns = {ard_da.name: 'vals'}).reset_index(drop = True)\n",
    "        #Reorganise data\n",
    "        df = df[ind_wider]\n",
    "        #Include original dataset attributes\n",
    "        df = pd.concat([df, da_attrs], axis = 1)\n",
    "        #Saving data frame\n",
    "        df.to_parquet(file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da54693-8145-433b-981f-150c2523858a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Extracting data from all model outputs for all FishMIP regional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d25c23-56e6-4948-be3d-c86570e69e1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jobfs/124565005.gadi-pbs/ipykernel_1273498/3346194176.py:43: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, '360_day', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  new_time = da.indexes['time'].to_datetimeindex()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rechunking /g/data/vf71/fishmip_inputs/ISIMIP3a/regional_inputs/obsclim/025deg/gfdl-mom6-cobalt2_obsclim_thetao_15arcmin_north-sea-ewe-mizer_monthly_1961_2010.zarr.\n",
      "Dimension \"lat\" has unequal chunks.\n",
      "Rechunking /g/data/vf71/fishmip_inputs/ISIMIP3a/regional_inputs/obsclim/025deg/gfdl-mom6-cobalt2_obsclim_thetao_15arcmin_north-sea-osmose_monthly_1961_2010.zarr.\n",
      "Dimension \"lat\" has unequal chunks.\n"
     ]
    }
   ],
   "source": [
    "for f in list_files:\n",
    "    #Open data array as ARD\n",
    "    da = open_ard_data(f, mask_ras)\n",
    "\n",
    "    #Create file name based on presence of depth dimension\n",
    "    if 'depth_bin_m' in da.dims:\n",
    "        base_file_out = os.path.basename(f).replace('.nc', '.zarr')\n",
    "    else:\n",
    "        base_file_out = os.path.basename(f).replace('.nc', '.parquet')\n",
    "    #Adding output folder to create full file path\n",
    "    base_file_out = os.path.join(base_out, base_file_out)\n",
    "\n",
    "    #Extract data for each region included in the regional mask\n",
    "    for i in rmes.region:\n",
    "        #Get polygon for each region\n",
    "        mask = rmes[rmes.region == i]\n",
    "        #Get name of region and clean it for use in output file\n",
    "        reg_name = mask['region'].values[0].lower().replace(\" \", \"-\").replace(\"'\", \"\")\n",
    "        #File name out - Replacing \"global\" for region name\n",
    "        file_out = base_file_out.replace('global', reg_name)\n",
    "        #Extract data and save masked data - but only if file does not already exist\n",
    "        if os.path.isdir(file_out) | os.path.isfile(file_out):\n",
    "            continue\n",
    "        mask_ard_data(da, mask, file_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-24.04] *",
   "language": "python",
   "name": "conda-env-analysis3-24.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
